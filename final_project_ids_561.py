# -*- coding: utf-8 -*-
"""Final Project IDS 561

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10w80eGf3mRelnpiGzyputmr6hVgx2W1w
"""

#Installing spark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
!tar xf spark-2.4.5-bin-hadoop2.7.tgz
!pip install -q findspark

#setting environment
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7.tgz"

#building session
import findspark
findspark.init('/content/spark-2.4.5-bin-hadoop2.7')
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Commented out IPython magic to ensure Python compatibility.
#mounting drive
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My\ Drive/Colab\ Notebooks

#Downloading products ratings data
!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Clothing_Shoes_and_Jewelry.csv

#Downloading metadata for the products
!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Clothing_Shoes_and_Jewelry.json.gz

"""## **Data Pre-Processing**"""

#importing libraries
import pandas as pd
import gzip
import json

#Coverting the metadata file from json to DataFrame
def parse(path):
  g = gzip.open(path, 'rb')
  for l in g:
    yield json.loads(l)

def getDF(path):
  i = 0
  df = {}
  for d in parse(path):
    df[i] = d
    i += 1
  return pd.DataFrame.from_dict(df, orient='index')

Beauty_metaData = getDF('meta_Clothing_Shoes_and_Jewelry.json.gz')

#looking at the metadata
Beauty_metaData.head()

#Checking null values in the columns
Beauty_metaData.isna().sum()

#Choosing only asin, title and main_cat as they have the least missing values and are useful for our later analysis.
Beauty_metaData1 = Beauty_metaData[["asin", "title", "main_cat"]]

#Checking the data again
Beauty_metaData1.head(40)

#dropping all the rows with missing values from meta data
Products_meta_df = Beauty_metaData1.dropna()
display(Products_meta_df)

#renaming the columns
Products_meta_df.columns = ["itemID", "title", "category"]
display(Products_meta_df)

#reading the ratings csv
Products_df = pd.read_csv("Clothing_Shoes_and_Jewelry.csv", header=None, names=["itemID", "userID", "rating"])

#checking the format
display(Products_df)

#Reducing the rows to 100,000
Products_df = Products_df.sample(n=100000, random_state=666)

#Importing library for encoding user_id and item_id
from sklearn import preprocessing as prep

#Encoding item_id in metadata
item_encoder = prep.LabelEncoder()
item_encoder.fit(Products_meta_df["itemID"])
encoded_items = item_encoder.transform(Products_meta_df["itemID"])

#Encoding user_id in ratings file
user_encoder = prep.LabelEncoder()
user_encoder.fit(Products_df["userID"])
encoded_users = user_encoder.transform(Products_df["userID"])

#Storing item_encoding as a dictionary
item_dict = {label:index for index, label in enumerate(item_encoder.classes_.tolist())}

count = len(item_dict)
Products_df["item_index"] = 0

unknown = []

for i in range(len(Products_df)):
  try:
    Products_df.iloc[i, 3] = item_dict[Products_df.iloc[i, 0]]
  except KeyError:
    count += 1
    unknown.append(Products_df.iloc[i, 0])
    item_dict[Products_df.iloc[i, 0]] = count
    Products_df.iloc[i, 3] = item_dict[Products_df.iloc[i, 0]]

#Creating dataframe consisting of unknown values to be merged with metadata
new_rows = [[itemID, "Unknown Item - " + str(itemID), "Others", item_dict[itemID]] for itemID in unknown if len(unknown) > 0 ]

if len(new_rows) > 0:
  new_rows = pd.DataFrame(data = new_rows, columns = ["itemID", "title", "category", "item_index"])

#Joining item_index to metadata as a column
encoded_items = pd.DataFrame(data = encoded_items.tolist(), columns=["item_index"])
Products_meta_df = pd.concat([Products_meta_df.reset_index(drop=True), encoded_items.reset_index(drop=True)], axis=1)

#Appending unknown items to the metadata
if len(unknown) > 0:
   Products_meta_df = Products_meta_df.append(new_rows)

#checking the metadata after processing
Products_meta_df

#Joining user_index to products_df as a column
encoded_users = pd.DataFrame(data = encoded_users.tolist(), columns=["user_index"])
Products_df = pd.concat([Products_df.reset_index(drop=True), encoded_users.reset_index(drop=True)], axis=1)

#Dropping unnecessary columns
Products_df = Products_df.drop(columns = ["itemID", "userID"])

#Exporting the product_df and metadata_Df to csv
Products_df.to_csv("Products_preprocessed.csv", sep = ",", index=False)
Products_meta_df.to_csv("Products_meta_preprocessed.csv", sep = ",", index=False)

#Storing item_encoding as a dictionary
user_dict = {label:index for index, label in enumerate(user_encoder.classes_.tolist())}

#Storing it as pickled/serialized object
import pickle
with open('user_dict.pkl', 'wb') as f:
  pickle.dump(user_dict, f, pickle.HIGHEST_PROTOCOL)

"""# **Model Building**"""

#importing necessary libraries for ALS
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS

#setting parameter grid for parameter tuning by cross-validation
evaluator = RegressionEvaluator(metricName = "rmse", labelCol = "rating",
                                predictionCol = "prediction")
#Model
recsys = ALS(userCol = "user_index", itemCol = "item_index",
             ratingCol = "rating", nonnegative = True, coldStartStrategy="drop")

#Parameter grid
paramGrid = ParamGridBuilder() \
    .addGrid(recsys.regParam, [0.1, 0.01, 0.001]) \
    .addGrid(recsys.rank, [5, 10, 15]) \
    .build()

#Cross validation 
cvs = CrossValidator(estimator = recsys,
                     estimatorParamMaps = paramGrid,
                     evaluator = evaluator,
                     numFolds=5)

#Reading the Products ratings file
pathToFile = 'Products_preprocessed.csv'
ProductsDF = spark.read.csv(pathToFile, inferSchema = True, header = True)
ProductsDF.printSchema()

#Reading the metadata file
pathToFile = 'Products_meta_preprocessed.csv'
metadataDF = spark.read.csv(pathToFile, inferSchema = True, header = True)
metadataDF.printSchema()

#cross validation on the products ratings data
cvs_model = cvs.fit(ProductsDF)

#defining best model 
recsys = cvs_model.bestModel

#Getting top 10 recommendations for all users
userRecs = recsys.recommendForAllUsers(10)
userRecs.show()

#printing schema for the recommendation engine
userRecs.printSchema()

#itertaing list of lists for recommedations
df = userRecs.toPandas()
list = df.values.tolist()
print(list[3])

#Converting list to dataframe
import numpy as np
new =[]
dfi = pd.DataFrame(columns = ['a','b','c'])
for i in list:
  for j in i[1]:
    #new= [int(x) for x in str(i[0])]
    new=[i[0]]
    #print(type(new))
    new.append(j[0])
    new.append(j[1])
    dfi = dfi.append({'a': new[0], 'b' : new[1], 'c' : new[2]} , ignore_index = True)

#checking the dataframe
dfi

#changing the datatype of columns
dfi[['b']] = dfi[['b']].astype('str')

#joining the dataframes metadata and ratings data to get the categories if recommended products 
join = pd.merge(dfi, metadataDF_panda, left_on = 'b', right_on = 'item_index', how = 'left', left_index = False, right_index = False )

#renaming columns
join = join.rename(columns={'a': 'user_id', 'c': 'rating'})
join

#converting the dataframe to csv
join.to_csv('joined.csv')